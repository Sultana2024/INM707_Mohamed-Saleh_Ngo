{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ce10f6-d55c-4681-9518-dddd8e03a2b7",
   "metadata": {},
   "source": [
    "# Mario: Basic Tasks \n",
    "\n",
    "#### Goal: Mario pick-up ladder, go to princess(goal) and avoid Bowser(obstacle) in 6x6 grid\n",
    "\n",
    "1. Environment definition\n",
    "2. State and Reward function\n",
    "3. Q-learning update rule and policy (epsilon greedy and softmax)\n",
    "4. Run and measure performance for one episode, 10, 50, 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18aca32-2a51-4d71-8e89-becb96baf762",
   "metadata": {},
   "source": [
    "* observation_space = state space as the grid is a fully observable environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1506b833-e80f-4289-a1dc-e9cf1aae5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "from gym import Env, logger, spaces, utils\n",
    "import random\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc37888-0173-4cfc-8a7a-4d3634accc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Warehouse is divided into a grid. Use these 'tiles' to represent the objects on the grid.\n",
    "class GridTile(Enum):\n",
    "    _Floor = 0\n",
    "    Mario = 1\n",
    "    Ladder = 2\n",
    "    Weapon = 3\n",
    "    Dragon = 4\n",
    "    Princess = 5\n",
    "\n",
    "    # Return the first letter of tile nme \n",
    "    def __str__(self):\n",
    "        return self.name[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee5e90d-40a1-4df9-9726-d4913c2004ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridEnv(gym.Env):\n",
    "    def __init__(self, gridsize=(6, 6), princess_position=(4, 4), dragon_position=(3,2),weapon_position=(4,5), ladder_position=(1,1)):\n",
    "        super(gridEnv, self).__init__()\n",
    "        \n",
    "        # Grid size 6x6\n",
    "        self.gridsize = gridsize\n",
    "        \n",
    "        # Position of goal: Princess Peach at (4,4)\n",
    "        self.princess_position = princess_position\n",
    "\n",
    "        # Position of obstacle: dragon at (3,2)\n",
    "        self.dragon_position = dragon_position\n",
    "        \n",
    "        # Position of weapon at (4,5)\n",
    "        self.weapon_position = weapon_position\n",
    "\n",
    "        # Position of tool: Ladder at (1,1)\n",
    "        self.ladder_position = ladder_position  \n",
    "        \n",
    "        # Define action space: up, down, left, right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Define observation space (state space): 0...5 rows/columns \n",
    "        # Each position of the agent in the grid is a state \n",
    "        self.observation_space = spaces.Box(low=0, high=max(self.gridsize), shape=(2,))\n",
    "        \n",
    "        # Define state where agent starts, here it's (1,0)\n",
    "        self.state = np.array([1, 0]) \n",
    "\n",
    "        # Initialise 6x6 grid where positions will be stored\n",
    "        self.grid = np.zeros(self.gridsize)  \n",
    "        \n",
    "        # Define ladder check\n",
    "        self.ladder_pickup = False\n",
    "        \n",
    "        # Define weapon check\n",
    "        self.weapon_pickup = False\n",
    "        \n",
    "        # Define if terminal state reached = learning episode ended\n",
    "        self.done = False \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the Mario's position and the grid\n",
    "        self.state = np.array([1, 0])  \n",
    "        self.done = False\n",
    "        \n",
    "        # Reset the grid (zeros) and place the goal position\n",
    "        self.grid = np.zeros(self.gridsize)\n",
    "        # Store positions, e.g. grid[4,4] = 2 is the princess position \n",
    "        self.grid[self.princess_position[0], self.princess_position[1]] = 2  \n",
    "        self.grid[self.dragon_position[0], self.dragon_position[1]] = 3\n",
    "        self.grid[self.ladder_position[0], self.ladder_position[1]] = 4\n",
    "        self.grid[self.weapon_position[0], self.weapon_position[1]] = 5\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Move the agent based on the action: up, down, left, right\n",
    "        if action == 0:  # Move up\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "        elif action == 1:  # Move down\n",
    "            self.state[0] = min(self.gridsize[0] - 1, self.state[0] + 1)\n",
    "        elif action == 2:  # Move left\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 3:  # Move right\n",
    "            self.state[1] = min(self.gridsize[1] - 1, self.state[1] + 1)\n",
    "                \n",
    "        # Compute reward and check if episode ends\n",
    "        reward, done = self.compute_reward()\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        # Check if the agent reached the goal\n",
    "        if np.array_equal(self.state, self.princess_position):\n",
    "            if self.ladder_pickup == True:\n",
    "                reward = 150\n",
    "                self.done = True\n",
    "            else:\n",
    "                reward = -150\n",
    "                \n",
    "        elif np.array_equal(self.state, self.weapon_position):\n",
    "            if self.weapon_pickup == False:\n",
    "                reward = 20\n",
    "                self.weapon_pickup == True\n",
    "\n",
    "        elif np.array_equal(self.state, self.dragon_position):\n",
    "            if self.weapon_pickup == True:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -50\n",
    "            \n",
    "        elif np.array_equal(self.state, self.ladder_position):\n",
    "            if self.ladder_pickup == False:\n",
    "                reward = 50 #small reward for picking up ladder\n",
    "                self.ladder_pickup == True\n",
    "        else: \n",
    "            reward = -1  #penalise if step taken with no win\n",
    "\n",
    "        return reward, self.done\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Clear the grid and update it with the agent's current position\n",
    "        self.grid = np.zeros(self.gridsize)\n",
    "        # Mario\n",
    "        self.grid[self.state[0], self.state[1]] = 1\n",
    "        # Princess Peach\n",
    "        self.grid[self.princess_position[0], self.princess_position[1]] = 2\n",
    "        \n",
    "        # Dragon 1,2\n",
    "        self.grid[self.dragon_position[0], self.dragon_position[1]] = 3\n",
    "        self.grid[self.ladder_position[0], self.ladder_position[1]] = 4\n",
    "        \n",
    "        # Plot the grid\n",
    "        plt.imshow(self.grid, cmap='Blues', interpolation='nearest')\n",
    "        #plt.xticks([])  \n",
    "        #plt.yticks([])  \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def render_2(self, mode='human'):\n",
    "        # Print current state on console\n",
    "        for r in range(self.gridsize[0]):\n",
    "            for c in range(self.gridsize[1]):\n",
    "                if ([r, c] == [self.state[0], self.state[1]]):\n",
    "                    print(GridTile.Mario, end=' ')\n",
    "                elif ([r,c] == [self.ladder_position[0],self.ladder_position[1]]):\n",
    "                    print(GridTile.Ladder, end=' ')\n",
    "                elif ([r,c] == [self.weapon_position[0],self.weapon_position[1]]):\n",
    "                    print(GridTile.Weapon, end=' ')\n",
    "                elif ([r,c] == [self.dragon_position[0], self.dragon_position[1]]):\n",
    "                    print(GridTile.Dragon, end=' ')\n",
    "                elif ([r,c] == [self.princess_position[0], self.princess_position[1]]):\n",
    "                    print(GridTile.Princess, end=' ')\n",
    "                else:\n",
    "                    print(GridTile._Floor, end=' ')\n",
    "\n",
    "            print() # new line\n",
    "        print() # new line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c81269a-94af-4baa-bcf4-3fa4fd28ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(q_table, state):\n",
    "  action = np.argmax(q_table[state])\n",
    "  #print(np.shape(action))\n",
    "  return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q_table, state, epsilon, grid_width):\n",
    "    flat_state = int(state[0]) * grid_width + int(state[1])  # Ensure integer index\n",
    "    random_int = random.uniform(0, 1)\n",
    "    if random_int > epsilon:\n",
    "        action = np.argmax(q_table[flat_state])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a499a9d6-dd00-47fe-9646-14c5736ca3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_index(state, width):\n",
    "    return state[0] * width + state[1]\n",
    "\n",
    "# Q-learning function \n",
    "def q_learning(env, alpha, gamma, epsilon, epsilon_decay, min_epsilon, episodes, total_reward):\n",
    "    \n",
    "    # Initialize Q-table with shape (36, 4) = (state, action)\n",
    "    q_table = np.zeros((env.gridsize[0] * env.gridsize[1], env.action_space.n))\n",
    "    #print(np.shape(q_table))\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        # Convert state to index\n",
    "        state_index = state_to_index(state, env.gridsize[1])\n",
    "        #print(state)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # Choose action based on epsilon-greedy strategy\n",
    "            action = epsilon_greedy_policy(q_table, state, epsilon, env.gridsize[1])\n",
    "\n",
    "            # Take the action and observe the result\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Convert next state to index\n",
    "            next_state_index = state_to_index(next_state, env.gridsize[1])  \n",
    "            \n",
    "            # Update Q-table based on the Q-learning formula\n",
    "            q_table[state_index, action] += alpha * (reward + gamma * np.max(q_table[next_state_index]) - q_table[state_index, action])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            # Update the state index\n",
    "            state_index = next_state_index \n",
    "            #env.render_2()\n",
    "\n",
    "            # Add up reward\n",
    "            total_reward +=reward\n",
    "\n",
    "            #print (done)\n",
    "            \n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        #epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*episodes)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        \n",
    "    return q_table, total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138f469-5952-4e8f-9410-996447eee3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.9\n",
    "gamma=0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay=0.995\n",
    "min_epsilon = 0.4\n",
    "episodes=1\n",
    "total_reward = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gridEnv()\n",
    "    # Start with new environment\n",
    "    state = env.reset()\n",
    "    # Show the environment\n",
    "    env.render_2()\n",
    "    q_table, total_reward = q_learning(env,alpha, gamma, epsilon, epsilon_decay, min_epsilon, episodes, total_reward)\n",
    "    print()\n",
    "    print(\"Trained Q-Table:\", q_table, '\\n total reward: ', total_reward)\n",
    "    #print(q_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b6f05-6a7a-4fa3-b10b-326eedba42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d69d50-2684-4929-83a9-14d36e27ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "gamma=0.99 \n",
    "tau = 1.0 \n",
    "episodes=1\n",
    "\n",
    "\n",
    "# Define softmax function \n",
    "def softmax(qvalue, tau):\n",
    "    exp_qvalue = np.exp(qvalue / tau)\n",
    "    return exp_qvalue / np.sum(exp_qvalue)\n",
    "    \n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Determine probabilities for each action at a given state \n",
    "        action_probability = softmax(q_table[state], tau)\n",
    "        \n",
    "        # Pick action randomly from action space with probabilities from softmax \n",
    "        # This ensures it will explore but the highest Q-value will have highest probs. \n",
    "        action = np.random.choice(np.arrange(env.action_space.n), p=action_probability)\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Update q_table\n",
    "        q_table[state, action] += alpha * (reward + gamma * (np.max(q_table[next_state])) - q_table[state, action])\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gridEnv()\n",
    "    state = env.reset()\n",
    "    env.render_2()\n",
    "    q_table = q_learning(env,alpha, gamma, epsilon, max_epsilon, epsilon_decay, min_epsilon, episodes)\n",
    "    print()\n",
    "    print(\"Trained Q-Table:\")\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41664c6-0d0e-4591-b027-3eb45020ddd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf304b-53d7-48c6-8f22-e09bde18e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ba6bb-99bb-4aa2-b910-329a504cdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "\n",
    "# --- Settings ---\n",
    "GRID_SIZE = 6\n",
    "CELL_SIZE = 80\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "GRAY = (200, 200, 200)\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Mario GridWorld\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Load Mario icon\n",
    "mario_img = pygame.image.load(\"mario.png\")\n",
    "mario_img = pygame.transform.scale(mario_img, (CELL_SIZE, CELL_SIZE))\n",
    "\n",
    "# Agent's initial position\n",
    "agent_pos = [0, 0]  # [row, col]\n",
    "\n",
    "def draw_grid():\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        for y in range(0, HEIGHT, CELL_SIZE):\n",
    "            rect = pygame.Rect(x, y, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(screen, GRAY, rect, 1)\n",
    "\n",
    "def draw_agent():\n",
    "    row, col = agent_pos\n",
    "    x = col * CELL_SIZE\n",
    "    y = row * CELL_SIZE\n",
    "    screen.blit(mario_img, (x, y))\n",
    "\n",
    "# --- Game loop ---\n",
    "running = True\n",
    "while running:\n",
    "    clock.tick(FPS)\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    # Handle events\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_UP and agent_pos[0] > 0:\n",
    "                agent_pos[0] -= 1\n",
    "            elif event.key == pygame.K_DOWN and agent_pos[0] < GRID_SIZE - 1:\n",
    "                agent_pos[0] += 1\n",
    "            elif event.key == pygame.K_LEFT and agent_pos[1] > 0:\n",
    "                agent_pos[1] -= 1\n",
    "            elif event.key == pygame.K_RIGHT and agent_pos[1] < GRID_SIZE - 1:\n",
    "                agent_pos[1] += 1\n",
    "\n",
    "    # Draw\n",
    "    draw_grid()\n",
    "    draw_agent()\n",
    "    pygame.display.flip()\n",
    "\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c7330-8bcf-4d5d-a414-69d5e5978b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
