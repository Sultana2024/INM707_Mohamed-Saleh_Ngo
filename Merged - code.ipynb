{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ce10f6-d55c-4681-9518-dddd8e03a2b7",
   "metadata": {},
   "source": [
    "# Mario: Basic Tasks \n",
    "\n",
    "#### Goal: Mario pick-up ladder, go to princess(goal) and avoid Bowser(obstacle) in 6x6 grid\n",
    "\n",
    "1. Environment definition\n",
    "2. State and Reward function\n",
    "3. Q-learning update rule and policy (epsilon greedy and softmax)\n",
    "4. Run and measure performance for one episode, 10, 50, 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18aca32-2a51-4d71-8e89-becb96baf762",
   "metadata": {},
   "source": [
    "* observation_space = state space as the grid is a fully observable environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "1506b833-e80f-4289-a1dc-e9cf1aae5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "from gym import Env, logger, spaces, utils\n",
    "import random\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "0bc37888-0173-4cfc-8a7a-4d3634accc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Warehouse is divided into a grid. Use these 'tiles' to represent the objects on the grid.\n",
    "class GridTile(Enum):\n",
    "    _Floor = 0\n",
    "    Mario = 1\n",
    "    Ladder = 2\n",
    "    Weapon = 3\n",
    "    Dragon = 4\n",
    "    Princess = 5\n",
    "\n",
    "    # Return the first letter of tile nme \n",
    "    def __str__(self):\n",
    "        return self.name[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "2ee5e90d-40a1-4df9-9726-d4913c2004ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridEnv(gym.Env):\n",
    "    def __init__(self, gridsize=(6, 6), princess_position=(4, 4), dragon_position=(3,2),weapon_position=(4,5), ladder_position=(1,1)):\n",
    "        super(gridEnv, self).__init__()\n",
    "        \n",
    "        # Grid size 6x6\n",
    "        self.gridsize = gridsize\n",
    "        \n",
    "        # Position of goal: Princess Peach at (4,4)\n",
    "        self.princess_position = princess_position\n",
    "\n",
    "        # Position of obstacle: dragon at (3,2)\n",
    "        self.dragon_position = dragon_position\n",
    "        \n",
    "        # Position of weapon at (4,5)\n",
    "        self.weapon_position = weapon_position\n",
    "\n",
    "        # Position of tool: Ladder at (1,1)\n",
    "        self.ladder_position = ladder_position  \n",
    "        \n",
    "        # Define action space: up, down, left, right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Define observation space (state space): 0...5 rows/columns \n",
    "        # Each position of the agent in the grid is a state \n",
    "        self.observation_space = spaces.Box(low=0, high=max(self.gridsize), shape=(2,))\n",
    "        \n",
    "        # Define state where agent starts, here it's (1,0)\n",
    "        self.state = np.array([1, 0]) \n",
    "\n",
    "        # Initialise 6x6 grid where positions will be stored\n",
    "        self.grid = np.zeros(self.gridsize)  \n",
    "        \n",
    "        # Define ladder check\n",
    "        self.ladder_pickup = False\n",
    "        \n",
    "        # Define weapon check\n",
    "        self.weapon_pickup = False\n",
    "        \n",
    "        # Define if terminal state reached = learning episode ended\n",
    "        self.done = False \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the Mario's position and the grid\n",
    "        self.state = np.array([1, 0])  \n",
    "        self.done = False\n",
    "        \n",
    "        # Reset the grid (zeros) and place the goal position\n",
    "        self.grid = np.zeros(self.gridsize)\n",
    "        # Store positions, e.g. grid[4,4] = 2 is the princess position \n",
    "        self.grid[self.princess_position[0], self.princess_position[1]] = 2  \n",
    "        self.grid[self.dragon_position[0], self.dragon_position[1]] = 3\n",
    "        self.grid[self.ladder_position[0], self.ladder_position[1]] = 4\n",
    "        self.grid[self.weapon_position[0], self.weapon_position[1]] = 5\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Move the agent based on the action: up, down, left, right\n",
    "        if action == 0:  # Move up\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "        elif action == 1:  # Move down\n",
    "            self.state[0] = min(self.gridsize[0] - 1, self.state[0] + 1)\n",
    "        elif action == 2:  # Move left\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 3:  # Move right\n",
    "            self.state[1] = min(self.gridsize[1] - 1, self.state[1] + 1)\n",
    "                \n",
    "        # Compute reward and check if episode ends\n",
    "        reward = self.compute_reward()\n",
    "        \n",
    "        return self.state, reward, self.done\n",
    "\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        # Check if the agent reached the goal\n",
    "        if np.array_equal(self.state, self.princess_position):\n",
    "            if self.ladder_pickup == True:\n",
    "                reward = 150\n",
    "                self.done = True\n",
    "            else:\n",
    "                reward = -150\n",
    "                \n",
    "        elif np.array_equal(self.state, self.weapon_position):\n",
    "            if self.weapon_pickup == False:\n",
    "                reward = 20\n",
    "                self.weapon_pickup == True\n",
    "\n",
    "        elif np.array_equal(self.state, self.dragon_position):\n",
    "            if self.weapon_pickup == True:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -50\n",
    "                self.done = True #Hitting dragon is a terminal state\n",
    "            \n",
    "        elif np.array_equal(self.state, self.ladder_position):\n",
    "            if self.ladder_pickup == False:\n",
    "                reward = 50 #small reward for picking up ladder\n",
    "                self.ladder_pickup == True\n",
    "        else: \n",
    "            reward = -1  #penalise if step taken with no win\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Clear the grid and update it with the agent's current position\n",
    "        self.grid = np.zeros(self.gridsize)\n",
    "        # Mario\n",
    "        self.grid[self.state[0], self.state[1]] = 1\n",
    "        # Princess Peach\n",
    "        self.grid[self.princess_position[0], self.princess_position[1]] = 2\n",
    "        \n",
    "        # Dragon 1,2\n",
    "        self.grid[self.dragon_position[0], self.dragon_position[1]] = 3\n",
    "        self.grid[self.ladder_position[0], self.ladder_position[1]] = 4\n",
    "        \n",
    "        # Plot the grid\n",
    "        plt.imshow(self.grid, cmap='Blues', interpolation='nearest')\n",
    "        #plt.xticks([])  \n",
    "        #plt.yticks([])  \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def render_2(self, mode='human'):\n",
    "        # Print current state on console\n",
    "        for r in range(self.gridsize[0]):\n",
    "            for c in range(self.gridsize[1]):\n",
    "                if ([r, c] == [self.state[0], self.state[1]]):\n",
    "                    print(GridTile.Mario, end=' ')\n",
    "                elif ([r,c] == [self.ladder_position[0],self.ladder_position[1]]):\n",
    "                    print(GridTile.Ladder, end=' ')\n",
    "                elif ([r,c] == [self.weapon_position[0],self.weapon_position[1]]):\n",
    "                    print(GridTile.Weapon, end=' ')\n",
    "                elif ([r,c] == [self.dragon_position[0], self.dragon_position[1]]):\n",
    "                    print(GridTile.Dragon, end=' ')\n",
    "                elif ([r,c] == [self.princess_position[0], self.princess_position[1]]):\n",
    "                    print(GridTile.Princess, end=' ')\n",
    "                else:\n",
    "                    print(GridTile._Floor, end=' ')\n",
    "\n",
    "            print() # new line\n",
    "        print() # new line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "9c81269a-94af-4baa-bcf4-3fa4fd28ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(q_table, state):\n",
    "  action = np.argmax(q_table[state])\n",
    "  #print(np.shape(action))\n",
    "  return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q_table, state, epsilon, grid_width):\n",
    "    flat_state = int(state[0]) * grid_width + int(state[1])  # Ensure integer index\n",
    "    random_int = random.uniform(0, 1)\n",
    "    if random_int > epsilon:\n",
    "        action = np.argmax(q_table[flat_state])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "a499a9d6-dd00-47fe-9646-14c5736ca3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_index(state, width):\n",
    "    return state[0] * width + state[1]\n",
    "\n",
    "# Q-learning function \n",
    "def q_learning(env, alpha, gamma, epsilon, max_epsilon, epsilon_decay, min_epsilon, episodes):\n",
    "    \n",
    "    # Initialize Q-table with shape (36, 4) = (state, action)\n",
    "    q_table = np.zeros((env.gridsize[0] * env.gridsize[1], env.action_space.n))\n",
    "    #print(np.shape(q_table))\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        # Convert state to index\n",
    "        state_index = state_to_index(state, env.gridsize[1])\n",
    "        #print(state)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # Choose action based on epsilon-greedy strategy\n",
    "            #max = q_table[state]\n",
    "            #print(state)\n",
    "            #print(q_table[state])\n",
    "            \n",
    "            action = epsilon_greedy_policy(q_table, state, epsilon, env.gridsize[1])\n",
    "            \n",
    "            #print(np.max(action))\n",
    "            #print(np.shape(action))\n",
    "            \n",
    "            # Take the action and observe the result\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = (next_state)\n",
    "            # Convert next state to index\n",
    "            next_state_index = state_to_index(next_state, env.gridsize[1])  \n",
    "            #print(q_table[next_state])\n",
    "            \n",
    "            # Update Q-table based on the Q-learning formula\n",
    "            q_table[state_index, action] += alpha * (reward + gamma * np.max(q_table[next_state_index]) - q_table[state_index, action])\n",
    "            #print(q_table)\n",
    "            #print(np.shape(q_table)[1])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            # Update the state index\n",
    "            state_index = next_state_index  \n",
    "        \n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        #epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*episodes)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "  \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "fb20d785-c0a6-499b-94d0-bf2cf6062e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ \n",
      "M L _ _ _ _ \n",
      "_ _ _ _ _ _ \n",
      "_ _ D _ _ _ \n",
      "_ _ _ _ P W \n",
      "_ _ _ _ _ _ \n",
      "\n",
      "\n",
      "Trained Q-Table:\n",
      "[[-1.00000000e-01 -1.00000000e-01 -1.00000000e-01  3.46179080e+00]\n",
      " [-1.00000000e-01  6.02524313e+01  0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e-01 -1.00000000e-01  1.57132957e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.90000000e-01 -6.53271998e-02  2.84212747e+01  2.90718907e+02]\n",
      " [ 3.05000000e-01  4.12700733e+00  2.64079197e+02 -1.90000000e-01]\n",
      " [-1.90000000e-01 -1.00000000e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e-01 -1.00000000e-01 -1.00000000e-01  7.79727608e+00]\n",
      " [ 8.13715628e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "min_epsilon = 0.05\n",
    "max_epsilon = 1.0\n",
    "epsilon_decay=0.0995\n",
    "epsilon = 0.1\n",
    "alpha=0.1\n",
    "gamma=0.99 \n",
    "episodes=1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gridEnv()\n",
    "    state = env.reset()\n",
    "    env.render_2()\n",
    "    q_table = q_learning(env,alpha, gamma, epsilon, max_epsilon, epsilon_decay, min_epsilon, episodes)\n",
    "    print()\n",
    "    print(\"Trained Q-Table:\")\n",
    "    print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "052b6f05-6a7a-4fa3-b10b-326eedba42cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00000000e-01 -1.00000000e-01 -1.00000000e-01  3.46179080e+00]\n",
      " [-1.00000000e-01  6.02524313e+01  0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e-01 -1.00000000e-01  1.57132957e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.90000000e-01 -6.53271998e-02  2.84212747e+01  2.90718907e+02]\n",
      " [ 3.05000000e-01  4.12700733e+00  2.64079197e+02 -1.90000000e-01]\n",
      " [-1.90000000e-01 -1.00000000e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e-01 -1.00000000e-01 -1.00000000e-01  7.79727608e+00]\n",
      " [ 8.13715628e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ba6bb-99bb-4aa2-b910-329a504cdaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
